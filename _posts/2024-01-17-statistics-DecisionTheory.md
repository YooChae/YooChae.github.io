---
title: "[PRML] Decision Theory"
excerpt: "Pattern Recognition and Machine Learning 1.5 ë‚´ìš© ì •ë¦¬"

categories:
  - statistics
tags:
  - [tag1,tag2]

permalink: /statistics/decision-theory/

toc: true
toc_sticky: true

date: 2024-01-17
last_modified_at: 2024-01-17
---
## Decision Theory

ë¶ˆí™•ì‹¤ì„± ì†ì—ì„œ Optimal Decisionì„ ê²°ì •í•  ìˆ˜ë„ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ decision theory

### 1. Motivation

Training Data (**x**, **t**)ê°€ ì£¼ì–´ì ¸ ìˆì„ ë•Œ 

- *p*(**x**, **t**) (â†’ dataì˜ ì™„ì „í•œ ë¶„í¬)ëŠ” uncertaintyì— ëŒ€í•œ ì™„ë²½í•œ ìš”ì•½ì„ ì œê³µ
    - ì ë¶„ì„ í†µí•´ posterior, likelihood êµ¬í•˜ê¸° ê°€ëŠ¥
    - *p*(**x**, **t**)ë¥¼ ì°¾ëŠ” ê³¼ì •(inference)ì€ ë§¤ìš° ì–´ë ¤ì›€
- í™˜ìì˜ ì•” ì—¬ë¶€ íŒë‹¨ decision ë‚´ë¦¬ê¸°
    
    $$
    Bayes'\:Thm. \rightarrow\: p(C_k|\textbf{x})=\dfrac{p(\textbf{x}|C_k)P(C_k)}{p(\textbf{x})}
    $$
    
    - ì•”ì´ ì¡´ì¬í•˜ë©´ t = 0 â†’ C1 class, ì•”ì´ ì—†ìœ¼ë©´ t = 1 â†’ C2 class ë¼ í•˜ì.
    - í•©ë¦¬ì ì¸ íŒë‹¨ ê²°ê³¼ëŠ” ì‚¬ì§„(**x**)ì„ ë³´ê³  ì•”ì¼ í™•ë¥ ê³¼ ì•„ë‹ í™•ë¥ ì„ ë¹„êµí•´ë³´ëŠ” ê²ƒ
        
        â†’ Posterior *p*(Ck | **x** )ê°€ í° ê²ƒì„ ê³ ë¥´ë©´ ë˜ì§€ ì•Šì„ê¹Œ? **2-1**ì—ì„œ ì¦ëª…í•´ë³´ì!
        

### 2-1. Minimizing the Misclassification Rate

**ëª©í‘œ: ì˜¤ë¶„ë¥˜ ì¤„ì´ê¸°**

- **Decision Region** R1, R2ë¥¼ ì¡ì•„ì„œ R1ì— í•´ë‹¹í•˜ëŠ” pointëŠ” ëª¨ë‘ C1ìœ¼ë¡œ, R2ì— í•´ë‹¹í•˜ëŠ” pointëŠ” C2ë¡œ ë¶„ë¥˜í•˜ì. Decision Regionë“¤ì˜ ê²½ê³„ë¥¼ **Decision Boundary**ë¼ê³  í•œë‹¤.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/26af9838-41a7-400d-9c93-f993565da096/ed036219-8a40-45fb-9c15-de9c701e700b/Untitled.png)

- ì˜¤ë¶„ë¥˜ ê³„ì‚°
    
    $$
    \begin{aligned}p(mistake) &= p(\textbf{x} \in R_1,C_2)+ p(\textbf{x} \in R_2,C_1)\\&= \int_{R_1}p(\textbf{x},C_2) d\textbf{x}+\int_{R_2}p(\textbf{x},C_1)d\textbf{x} \end{aligned}
    $$
    
    - ê·¸ë¦¼ìœ¼ë¡œ í™•ì¸í•´ë³´ë©´
        - x(hat)ì„ decision boundaryë¡œ ì¡ì•˜ì„ ë•Œ: ì˜¤ë¶„ë¥˜ = Red + Green + Blue
        - x_0ë¥¼ decision boundaryë¡œ ì¡ì•˜ì„ ë•Œ ì˜¤ë¶„ë¥˜ = Green + Blue â† ê°€ì¥ ìµœì†Œì¼ ë•Œ
    - ë”°ë¼ì„œ ì–´ë–¤ **x**ì— ëŒ€í•´ì„œ *p*(**x**, C1), *p*(**x**, C2) ì¤‘ ë” í° ê°’ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ìœ„ì˜ ì˜¤ë¶„ë¥˜ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.
    
    $$
    p(\textbf{x},C_k)=p(C_k|\textbf{x})p(\textbf{x})
    $$
    
    - *p*(**x**)ëŠ” ê³µí†µì´ê¸° ë•Œë¬¸ì— *p*(**x**, Ck)ê°€ í¬ë‹¤ëŠ” ê²ƒì€ posteriorì¸ *p*(Ck | **x**)ê°€ í° ê²ƒê³¼ ê°™ë‹¤.
- Multiclassë¡œ í™•ì¥
    
    $$
    p(correct) = \sum_{k=1}^Kp(\textbf{x}\in R_k, C_k) = \sum_{k=1}^K\int_{R_k}p(\textbf{x},C_k)d\textbf{x}
    $$
    
    $\therefore$ *p*(correct)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ ë˜ *p*(**x**, C_k)ê°€ í° classì— í• ë‹¹ë˜ë„ë¡ decision regionì„ ì„¤ì •
    

> **ìš”ì•½** ğŸ’¡
1. ì˜¤ë¶„ë¥˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì€ *p*(**x**, C_k)ì´ í° classì— ë°°ì •í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
2. ì´ëŠ” ê²°êµ­ *p*(C_k | **x**)ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
> 

### 2-2. Minimizing the Expected Loss

- ì˜¤ë¶„ë¥˜ë¥¼ ë°©ì§€í•˜ë˜ 1ì¢… ì˜¤ë¥˜ë³´ë‹¤ 2ì¢… ì˜¤ë¥˜ ê³ ë ¤ë¥¼ ë” ìš°ì„ ì‹œí•˜ê³  ì‹¶ë‹¤ë©´? â†’ Loss Matrix ë„ì…
- ë°œìƒ ê°€ëŠ¥í•œ Lossë¥¼ ì¤„ì¸ë‹¤ = Total Lossë¥¼ ì¤„ì¸ë‹¤ = Average Lossë¥¼ ì¤„ì¸ë‹¤
    
    $$
    E[L] = \sum _k\sum_j\int_{R_j}L_{kj}p(\textbf{x}, C_k)d\textbf{x}
    $$
    
    ë”°ë¼ì„œ ê° xì— ëŒ€í•´ $\sum_kL_{kj}p(\textbf{x}, C_k)$ â†’ $\sum_kL_{kj}p(C_k|\textbf{x})$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. 
    
    - Loss Matrix ì˜ˆì‹œ -> L_12 = 1000ìœ¼ë¡œ ì„¤ì •í•¨ìœ¼ë¡œì„œ 2ì¢… ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ìš°ì„ ì‹œí•˜ë„ë¡ í•¨
    
    | Loss Matrix | cancer | normal |
    | --- | --- | --- |
    | cancer | 0 | 1000 |
    | normal | 1 | 0 |

### 3. The Reject Option

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/26af9838-41a7-400d-9c93-f993565da096/f0452289-c829-4600-ad2e-2c47935de036/Untitled.png)

- Regionì˜ uncertaintyê°€ ë†’ì€ ê²½ìš° ë°ì´í„°ì˜ ë¶„ë¥˜ë¥¼ ìœ ë³´í•  ìˆ˜ ìˆë‹¤. threshold $\theta$ì— ëŒ€í•´ ë‹¤ìŒì„ ë§Œì¡±í•˜ë©´ ìœ ë³´í•œë‹¤.

$$
\max_k \{p(C_k|\textbf{x})\} \le \theta
$$

### 4. Inference and Decision

- ë¶„ë¥˜ë¬¸ì œëŠ” í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰  ìˆ˜ ìˆë‹¤.
    - Inference : train dataë¥¼ ì´ìš©í•œ posterior í•™ìŠµ
    - Decision : posterior ì´ìš©í•´ì„œ class í• ë‹¹

#### Decision Problem(Classification)ì„ ìœ„í•œ ë°©ë²•

1. Generative Models
    - Inference Stage : Likelihood, Priorë¥¼ ì°¾ì€ í›„ Bayesâ€™ Thm. ì´ìš©í•´ì„œ Posterior êµ¬í•˜ê¸°
    - Decision Stage : ìƒˆë¡œìš´ inputì— ëŒ€í•œ class í• ë‹¹
    - $p(\textbf{x}) = \sum_k p(\textbf{x}|c_k)p(C_k)$ ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ë¥ ì´ ë‚®ì€ xë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ
        
        â†’ outlier detection
        
    - ë³µì¡ë„ ë†’ê³  ëŒ€ëŸ‰ì˜ training data í•„ìš”
2. Discriminative Models
    - Inference Stage : posterior í•™ìŠµ
    - Decision Stage : ìƒˆë¡œìš´ inputì— ëŒ€í•œ class í• ë‹¹
    - ìƒì„±ëª¨ë¸ì´ ë¹„í•´ ë³µì¡ë„ ë‚®ì§€ë§Œ ì´ìƒì¹˜ íƒì§€ ì–´ë ¤ì›€
3. Linear Discriminative Models
    - Inference, Decisionì˜ êµ¬ë¶„ì´ ì—†ìŒ : input pointë¥¼ classë¡œ mappingí•˜ëŠ” discriminant functionì„ ë°”ë¡œ í•™ìŠµ
    - decision boundaryì¸ ì´ˆë¡ìƒ‰ ìˆ˜ì§ì„ ì„ ì°¾ìŒ
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/26af9838-41a7-400d-9c93-f993565da096/eca44dac-911f-4959-9e5d-77e92d48ee19/Untitled.png)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/26af9838-41a7-400d-9c93-f993565da096/36d0a739-a689-42cc-bcbe-dfc5945269d7/Untitled.png)
    

### 5. Loss Functions for Regression

Decision Theoryë¥¼ Regressionì— ì ìš©í•´ë³´ì.

- Decision Stage: input xì— ëŒ€í•˜ì—¬ tì— ëŒ€í•œ estimate y(x)ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒ: 2ê°€ì§€ ë°©ë²•
- Total Loss â†” Average Lossë¥¼ ì¤„ì´ëŠ” y(x)ë¥¼ ì„ íƒí•œë‹¤.
    
    $$
    E[L] = \int\int L(t, y(\textbf{x}))p(\textbf{x},t)d\textbf{x}dt\\E[L] = \int\int (y(\textbf{x})-t)^2p(\textbf{x},t)d\textbf{x}dt\\\dfrac{\delta E[L]}{\delta y(\textbf{x})}= 2\int \{y(x)-t\}p(\textbf{x},t)dt = 0\\\therefore y(\textbf{x})= \dfrac{\int tp(\textbf{x},t)dt}{p(\textbf{x})}=\int tp(t|\textbf{x})dt = E_t[t|\textbf{x}]
    $$
    
    â†’ xì— ëŒ€í•œ tì˜ ê°’ì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡ì„ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ (íšŒê·€ì˜ ê²°ê³¼ì™€ ë™ì¼)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/26af9838-41a7-400d-9c93-f993565da096/3681a4b0-8f86-4ac8-b06b-1bbffe0b5e76/Untitled.png)
    
    - ë§Œì•½ t ë˜í•œ ë²¡í„°ë¡œ ë‚˜ì˜¨ë‹¤ë©´ $y(\textbf{x})=E_t[\textbf{t}|\textbf{x}]$ ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

ë‹¤ë¥¸ ë°©ë©´ìœ¼ë¡œ ë³¸ë‹¤ë©´

$$
\{y(\textbf{x})-t\}^2 = \{y(\textbf{x})-E_t[t|\textbf{x}]+E_t[t|\textbf{x}]-t\}^2 \\=\{y(\textbf{x})-E_t[t|\textbf{x}]\}^2+2\{y(\textbf{x})-E_t[t|\textbf{x}]\}\{E_t[t|\textbf{x}]-t\}+\{E_t[t|\textbf{x}]-t\}^2
$$

$$
\therefore E[L] = \int\int (y(\textbf{x})-t)^2p(\textbf{x},t)d\textbf{x}dt \\=\int \{y(\textbf{x})-E_t[t|\textbf{x}]\}^2p(\textbf{x})d\textbf{x}+\int \{E_t[t|\textbf{x}]-t\}^2p(\textbf{x})d\textbf{x}\\=\int \{y(\textbf{x})-E_t[t|\textbf{x}]\}^2p(\textbf{x})d\textbf{x}+\int var [t|\textbf{x}]p(\textbf{x})d\textbf{x}
$$

- ë”°ë¼ì„œ *E*[**L**]ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ y(x) = *E*[t | **x**]ë¥¼ ì„ íƒí•´ì•¼ í•˜ë©° ìš°ì¸¡í•­ì€ ì§€ì›Œì§€ì§€ ì•Šê¸°ì— target data ë‚´ì— ë‚´ì¬ëœ noiseë¡œ, E[L]ì˜ ìµœì†Ÿê°’ì´ ëœë‹¤.
- (í™•ì¥)Decision Problem(Regression)ì„ ìœ„í•œ ë°©ë²•

$$
E[L_q] = \int\int (y(\textbf{x})-t)^qp(\textbf{x},t)d\textbf{x}dt
$$

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/26af9838-41a7-400d-9c93-f993565da096/807c02fb-42c9-44b3-a057-3e5e03fd5c39/Untitled.png)


#### Decision Theory(Regression)ì„ ìœ„í•œ ë°©ë²•

1. Inference stage: *p*(**x**, t)ë¥¼ êµ¬í•¨ â†’ ì´í›„ *p*(t | **x**)ë¥¼ ì°¾ì•„ *E*[t | **x**] ê³„ì‚°
2. Inference stage: *p*(t | **x**)ë¥¼ êµ¬í•¨ â†’ ì´í›„ *E*[t | **x**] ê³„ì‚°
3. Training Dataë¡œë¶€í„° y(x) functionì„ êµ¬í•¨